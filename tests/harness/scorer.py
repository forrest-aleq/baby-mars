"""
Test Case Scorer
==================

Computes scores for test cases based on actual vs expected behavior.

Scoring Dimensions (per plan):
- Understanding (20%): Appraisal accuracy
- Action Correctness (30%): Work unit accuracy
- Autonomy Calibration (20%): Supervision mode match
- Completeness (20%): All expected work units generated
- Error Handling (10%): Escalation accuracy

Pass threshold: 96%
"""

from typing import Optional
from .schema import (
    TestCase,
    TestCaseResult,
    PersonaResult,
    CompanyResult,
    HarnessReport,
    ScoreBreakdown,
    ExpectedWorkUnit,
)


class Scorer:
    """Scores test results against expectations."""

    def __init__(self, pass_threshold: float = 96.0):
        self.pass_threshold = pass_threshold

    def score_test_case(
        self,
        test_case: TestCase,
        actual_supervision_mode: str,
        actual_work_units: list[dict],
        actual_appraisal: Optional[dict] = None,
        did_escalate: bool = False,
        flagged_exceptions: bool = False,
    ) -> TestCaseResult:
        """
        Score a single test case execution.

        Args:
            test_case: The test case specification
            actual_supervision_mode: What mode the agent chose
            actual_work_units: Work units generated by the agent
            actual_appraisal: Appraisal output (for understanding score)
            did_escalate: Whether agent escalated to human
            flagged_exceptions: Whether agent flagged exceptions

        Returns:
            TestCaseResult with breakdown scores
        """
        expected = test_case.expected
        breakdown = ScoreBreakdown()
        errors = []

        # === 1. Understanding (20%) ===
        # Check if appraisal correctly identified difficulty and concerns
        understanding_score = 100.0  # Start with full score

        if actual_appraisal:
            # Check ethical concern detection
            has_ethical_concerns = actual_appraisal.get("ethical_concerns", False)
            if expected.ethical_concerns != has_ethical_concerns:
                understanding_score -= 50.0
                errors.append(
                    f"Ethical detection: expected {expected.ethical_concerns}, got {has_ethical_concerns}"
                )

        breakdown.understanding = max(0.0, understanding_score)

        # === 2. Action Correctness (30%) ===
        # Check if work units match expected tool/verb patterns
        action_score = self._score_work_unit_correctness(
            expected.work_units, actual_work_units, errors
        )
        breakdown.action_correctness = action_score

        # === 3. Autonomy Calibration (20%) ===
        # Check if supervision mode matches
        if actual_supervision_mode == expected.supervision_mode:
            breakdown.autonomy_calibration = 100.0
        else:
            breakdown.autonomy_calibration = 0.0
            errors.append(
                f"Supervision mode: expected {expected.supervision_mode}, got {actual_supervision_mode}"
            )

        # === 4. Completeness (20%) ===
        # Check if all expected work units were generated
        completeness_score = self._score_completeness(
            expected.work_units, actual_work_units, errors
        )
        breakdown.completeness = completeness_score

        # === 5. Error Handling (10%) ===
        # Check escalation and exception flagging
        error_handling_score = 100.0

        if expected.should_escalate != did_escalate:
            error_handling_score -= 50.0
            errors.append(
                f"Escalation: expected {expected.should_escalate}, got {did_escalate}"
            )

        if expected.should_flag_exceptions != flagged_exceptions:
            error_handling_score -= 50.0
            errors.append(
                f"Exception flagging: expected {expected.should_flag_exceptions}, got {flagged_exceptions}"
            )

        breakdown.error_handling = max(0.0, error_handling_score)

        # === Calculate Total ===
        total_score = breakdown.weighted_total()
        passed = total_score >= self.pass_threshold

        return TestCaseResult(
            test_case_id=test_case.id,
            passed=passed,
            score=total_score,
            breakdown=breakdown,
            actual_supervision_mode=actual_supervision_mode,
            actual_work_units=actual_work_units,
            errors=errors,
        )

    def _score_work_unit_correctness(
        self,
        expected_units: list[ExpectedWorkUnit],
        actual_units: list[dict],
        errors: list[str],
    ) -> float:
        """Score how well actual work units match expected patterns."""
        if not expected_units:
            # No expectations = 100% if no work units, 50% if some generated
            return 100.0 if not actual_units else 50.0

        total_score = 0.0
        checks = 0

        for expected in expected_units:
            checks += 1

            # Count actual units matching this pattern
            matching = [
                u for u in actual_units
                if u.get("tool", "").lower() == expected.tool.lower()
                and u.get("verb", "").lower() == expected.verb.lower()
            ]

            count = len(matching)

            if count >= expected.min_count:
                if expected.max_count is None or count <= expected.max_count:
                    total_score += 100.0
                else:
                    # Over max count - partial credit
                    total_score += 50.0
                    errors.append(
                        f"Too many {expected.tool}.{expected.verb}: expected max {expected.max_count}, got {count}"
                    )
            else:
                # Under min count - proportional credit
                ratio = count / expected.min_count if expected.min_count > 0 else 0
                total_score += ratio * 100.0
                errors.append(
                    f"Not enough {expected.tool}.{expected.verb}: expected min {expected.min_count}, got {count}"
                )

        return total_score / checks if checks > 0 else 100.0

    def _score_completeness(
        self,
        expected_units: list[ExpectedWorkUnit],
        actual_units: list[dict],
        errors: list[str],
    ) -> float:
        """Score whether all expected work units were generated."""
        if not expected_units:
            return 100.0

        found = 0
        total = len(expected_units)

        for expected in expected_units:
            matching = [
                u for u in actual_units
                if u.get("tool", "").lower() == expected.tool.lower()
                and u.get("verb", "").lower() == expected.verb.lower()
            ]
            if len(matching) >= expected.min_count:
                found += 1

        return (found / total) * 100.0 if total > 0 else 100.0

    def aggregate_persona_results(
        self,
        persona_name: str,
        company: str,
        test_results: list[TestCaseResult],
        execution_time_ms: float = 0.0,
    ) -> PersonaResult:
        """Aggregate test case results into persona result."""
        passed = sum(1 for r in test_results if r.passed)
        failed = len(test_results) - passed

        # Average score across all tests
        if test_results:
            avg_score = sum(r.score for r in test_results) / len(test_results)
        else:
            avg_score = 0.0

        return PersonaResult(
            persona_name=persona_name,
            company=company,
            total_tests=len(test_results),
            passed_tests=passed,
            failed_tests=failed,
            skipped_tests=0,
            score=avg_score,
            test_results=test_results,
            execution_time_ms=execution_time_ms,
        )

    def aggregate_company_results(
        self,
        company: str,
        persona_results: list[PersonaResult],
    ) -> CompanyResult:
        """Aggregate persona results into company result."""
        total_tests = sum(p.total_tests for p in persona_results)
        passed_tests = sum(p.passed_tests for p in persona_results)

        # Average score across all personas
        if persona_results:
            avg_score = sum(p.score for p in persona_results) / len(persona_results)
        else:
            avg_score = 0.0

        return CompanyResult(
            company=company,
            total_personas=len(persona_results),
            total_tests=total_tests,
            passed_tests=passed_tests,
            score=avg_score,
            persona_results=persona_results,
        )

    def generate_report(
        self,
        company_results: list[CompanyResult],
        execution_time_ms: float = 0.0,
    ) -> HarnessReport:
        """Generate final harness report."""
        total_personas = sum(c.total_personas for c in company_results)
        total_tests = sum(c.total_tests for c in company_results)
        passed_tests = sum(c.passed_tests for c in company_results)
        failed_tests = total_tests - passed_tests

        # Overall score = average of company scores
        if company_results:
            overall_score = sum(c.score for c in company_results) / len(company_results)
        else:
            overall_score = 0.0

        # Collect top failures
        top_failures = []
        for cr in company_results:
            for pr in cr.persona_results:
                for tr in pr.test_results:
                    if not tr.passed:
                        top_failures.append({
                            "persona": pr.persona_name,
                            "test_case": tr.test_case_id,
                            "score": tr.score,
                            "errors": tr.errors[:2],  # Top 2 errors
                        })

        # Sort by score (lowest first) and take top 10
        top_failures.sort(key=lambda x: x["score"])
        top_failures = top_failures[:10]

        # Generate recommendations based on failures
        recommendations = self._generate_recommendations(company_results, top_failures)

        return HarnessReport(
            overall_score=overall_score,
            pass_threshold=self.pass_threshold,
            passed=overall_score >= self.pass_threshold,
            total_personas=total_personas,
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            company_results=company_results,
            top_failures=top_failures,
            recommendations=recommendations,
            execution_time_ms=execution_time_ms,
        )

    def _generate_recommendations(
        self,
        company_results: list[CompanyResult],
        top_failures: list[dict],
    ) -> list[str]:
        """Generate recommendations based on failure patterns."""
        recommendations = []

        # Analyze failure patterns
        supervision_mismatches = 0
        missing_work_units = 0
        escalation_issues = 0

        for cr in company_results:
            for pr in cr.persona_results:
                for tr in pr.test_results:
                    for error in tr.errors:
                        if "Supervision mode" in error:
                            supervision_mismatches += 1
                        if "Not enough" in error or "Too many" in error:
                            missing_work_units += 1
                        if "Escalation" in error:
                            escalation_issues += 1

        if supervision_mismatches > 5:
            recommendations.append(
                f"Tune autonomy thresholds - {supervision_mismatches} supervision mode mismatches"
            )

        if missing_work_units > 5:
            recommendations.append(
                f"Improve action planning - {missing_work_units} work unit count issues"
            )

        if escalation_issues > 3:
            recommendations.append(
                f"Calibrate escalation logic - {escalation_issues} escalation mismatches"
            )

        if not recommendations:
            recommendations.append("No major issues detected")

        return recommendations
